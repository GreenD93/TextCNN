{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os\n",
    "import params\n",
    "\n",
    "embed_lookup = KeyedVectors.load_word2vec_format(params.w2v_path, \n",
    "                                                 binary=True)\n",
    "\n",
    "print('변경 전')\n",
    "print(embed_lookup.vectors.shape)\n",
    "\n",
    "# \n",
    "# # <pad> vector, index 추가\n",
    "# \n",
    "\n",
    "pad_vectors = np.zeros_like(embed_lookup.vectors[0])\n",
    "embed_lookup.vectors = np.insert(embed_lookup.vectors, 0, pad_vectors,axis=0)\n",
    "embed_lookup.index2word.insert(0,'<pad>')\n",
    "\n",
    "print('변경 후')\n",
    "print(embed_lookup.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store pretrained vocab\n",
    "pretrained_words = []\n",
    "for word in embed_lookup.vocab:\n",
    "    pretrained_words.append(word)\n",
    "\n",
    "pretrained_words.insert(0,'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocab: 41722\n",
      "\n",
      "Word in vocab: g\n",
      "\n",
      "Length of embedding: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_idx = 1\n",
    "\n",
    "# get word/embedding in that row\n",
    "word = pretrained_words[row_idx] # get words by index\n",
    "embedding = embed_lookup[word] # embeddings by word\n",
    "\n",
    "# vocab and embedding info\n",
    "print(\"Size of Vocab: {}\\n\".format(len(pretrained_words)))\n",
    "print('Word in vocab: {}\\n'.format(word))\n",
    "print('Length of embedding: {}\\n'.format(len(embedding)))\n",
    "#print('Associated embedding: \\n', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prodNm</th>\n",
       "      <th>cleaned_prodNm</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>피아토스 감자칩 바베큐맛 85g 피아토스,감자칩,수입감자칩,수입과자</td>\n",
       "      <td>피아토스 감자칩 바베큐맛 g 피아토스 감자칩 수입감자칩 수입과자</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>청우 왕사탕 500g/사탕/왕사탕/캔디/왕캔디/간식</td>\n",
       "      <td>청우 왕사탕 g 사탕 왕사탕 캔디 왕캔디 간식</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACER Swift3 SF314-52G-59WM용 저반사필름</td>\n",
       "      <td>acer swift sf - g- wm용 저반사필름</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>앤디스 크림 데 민트 띤 132g 고급초콜렛,수입초콜렛,수입사탕</td>\n",
       "      <td>앤디스 크림 데 민트 띤 g 고급초콜렛 수입초콜렛 수입사탕</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>맥심 오리지널 20T 24입 커피/차/꿀 무료배송</td>\n",
       "      <td>맥심 오리지널 t 입 커피 차 꿀 무료배송</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  prodNm                       cleaned_prodNm  \\\n",
       "0  피아토스 감자칩 바베큐맛 85g 피아토스,감자칩,수입감자칩,수입과자  피아토스 감자칩 바베큐맛 g 피아토스 감자칩 수입감자칩 수입과자   \n",
       "1           청우 왕사탕 500g/사탕/왕사탕/캔디/왕캔디/간식            청우 왕사탕 g 사탕 왕사탕 캔디 왕캔디 간식   \n",
       "2      ACER Swift3 SF314-52G-59WM용 저반사필름         acer swift sf - g- wm용 저반사필름   \n",
       "3    앤디스 크림 데 민트 띤 132g 고급초콜렛,수입초콜렛,수입사탕     앤디스 크림 데 민트 띤 g 고급초콜렛 수입초콜렛 수입사탕   \n",
       "4            맥심 오리지널 20T 24입 커피/차/꿀 무료배송              맥심 오리지널 t 입 커피 차 꿀 무료배송   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      1  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_df = pd.read_csv('data/tarin_prodNm.csv', encoding='euc-kr')\n",
    "train_df = shuffle(train_df, random_state=33).reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_prodNm = train_df['cleaned_prodNm'].values.tolist()\n",
    "encoded_labels = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4029, 480, 977, 1, 4029, 480, 6626, 183]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert prodNm to tokens\n",
    "\n",
    "def tokenize_all_prodNm(embed_lookup, cleaned_prodNm):\n",
    "    \n",
    "    # split each prodNm into a list of words\n",
    "    prodNm_words = [prodNm.split() for prodNm in cleaned_prodNm]\n",
    "\n",
    "    tokenized_prodNms = []\n",
    "    for prodNm in prodNm_words:\n",
    "        ints = []\n",
    "        for word in prodNm:\n",
    "            try:\n",
    "                idx = embed_lookup.vocab[word].index +1\n",
    "            except: \n",
    "                idx = 0\n",
    "            ints.append(idx)\n",
    "            \n",
    "        tokenized_prodNms.append(ints)\n",
    "    \n",
    "    return tokenized_prodNms\n",
    "\n",
    "tokenize_all_cleaned_prodNms = tokenize_all_prodNm(embed_lookup, cleaned_prodNm)\n",
    "tokenize_all_cleaned_prodNms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4029, 480, 977, 1, 4029, 480, 6626, 183]\n"
     ]
    }
   ],
   "source": [
    "# testing code and printing a tokenized review\n",
    "print(tokenize_all_cleaned_prodNms[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0, 4029,  480,  977,    1,\n",
       "       4029,  480, 6626,  183])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill padding into toekized_prodNms\n",
    "\n",
    "def pad_features(tokenize_all_cleaned_prodNms, seq_length):\n",
    "    ''' Return features of tokenized_prodNms, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tokenize_all_cleaned_prodNms), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(tokenize_all_cleaned_prodNms):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = pad_features(tokenize_all_cleaned_prodNms,15)\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(11411, 15) \n",
      "Validation set: \t(1426, 15) \n",
      "Test set: \t\t(1427, 15)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data save\n",
    "np.savez('data/food_cat_prdNm.npz',\n",
    "         train_x=train_x, train_y=train_y, \n",
    "         val_x=val_x, val_y=val_y, \n",
    "         test_x=test_x, test_y=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'food_cat_prdNm.npz'\n",
    "with np.load(path) as data:\n",
    "    train_x = data['train_x']\n",
    "    train_y = data['train_y']\n",
    "    val_x = data['val_x']\n",
    "    val_y = data['val_y']\n",
    "    test_x = data['test_x']\n",
    "    test_y = data['test_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtCNN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model that will be used to perform prodNm analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim, feature_size,\n",
    "                num_filters=100, kernel_sizes=[3,4,5], freeze_embeddings= True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(TxtCNN, self).__init__()\n",
    "        \n",
    "        # set class vars\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.kernerl_sizes = kernel_sizes\n",
    "        self.feature_size = feature_size\n",
    "        \n",
    "        # 1. embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, \n",
    "                                              embedding_dim,\n",
    "                                              embeddings_initializer=tf.keras.initializers.Constant(embed_lookup.vectors),\n",
    "                                              trainable = freeze_embeddings\n",
    "                                             )\n",
    "        # 2. convolutional layers\n",
    "        self.conv_layers = []\n",
    "        \n",
    "        for kernerl_size in self.kernerl_sizes:\n",
    "            \"\"\"\n",
    "            Convolutional + max pooling layer\n",
    "            \"\"\"\n",
    "            conv_block = tf.keras.Sequential()\n",
    "            kernel_shape = (kernerl_size, embedding_dim)\n",
    "                \n",
    "            conv =  tf.keras.layers.Conv2D(filters = 100, \n",
    "                                        kernel_size=kernel_shape, \n",
    "                                        padding='valid',\n",
    "                                        strides = (1,1), \n",
    "                                        activation='relu',\n",
    "                                        name = 'conv_layer_{0}'.format(kernerl_size))\n",
    "            \n",
    "            maxpool = tf.keras.layers.MaxPool2D(pool_size = (self.feature_size - kernerl_size + 1, 1),\n",
    "                                                padding = 'valid',\n",
    "                                                strides = (1,1),\n",
    "                                                name = 'maxPool_layer_{0}'.format(kernerl_size))\n",
    "            \n",
    "            conv_block.add(conv)\n",
    "            conv_block.add(maxpool)\n",
    "            \n",
    "            self.conv_layers.append(conv_block)\n",
    "        \n",
    "        # 3. final fully-connected layer for classification\n",
    "        self.fc = tf.keras.layers.Dense(output_size, \n",
    "                                        activation='sigmoid',\n",
    "                                        name='predictions')\n",
    "\n",
    "        # 4. reshape and flatten & dropout layers\n",
    "        self.reshape = tf.keras.layers.Reshape((feature_size, embedding_dim, 1))\n",
    "        self.flatten = tf.keras.layers.Flatten(name='flatten')\n",
    "        self.dropout = tf.keras.layers.Dropout(drop_prob, name='dropout')\n",
    "        \n",
    "    \n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Defines how a batch of inputs, x, passes through the model layers.\n",
    "        Returns a single, sigmoid-activated class score as output.\n",
    "        \"\"\"\n",
    "        \n",
    "        embeds = self.embedding(inputs)\n",
    "        embeds = self.reshape(embeds)\n",
    "        pool_outputs = [conv_block(embeds) for conv_block in self.conv_layers]\n",
    "        \n",
    "        pool_outputs = tf.keras.layers.concatenate(pool_outputs, axis=-1, name='concatenate')\n",
    "        pool_outputs = self.flatten(pool_outputs)\n",
    "\n",
    "        if training:\n",
    "            pool_outputs = self.dropout(pool_outputs)\n",
    "            \n",
    "        logit = self.fc(pool_outputs)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(pretrained_words)\n",
    "output_size = 1 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[1]]) # 200-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "feature_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "# multi gpu usage\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    net = TxtCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   feature_size, num_filters, kernel_sizes)\n",
    "    \n",
    "    # Specify the training configuration (optimizer, loss, metrics)\n",
    "\n",
    "    net.compile(optimizer=tf.optimizers.Adam(),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Train for 179 steps, validate for 23 steps\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "179/179 [==============================] - 5s 31ms/step - loss: 0.2405 - accuracy: 0.9069 - val_loss: 0.1635 - val_accuracy: 0.9362\n",
      "Epoch 2/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.1426 - accuracy: 0.9475 - val_loss: 0.1223 - val_accuracy: 0.9537\n",
      "Epoch 3/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0889 - accuracy: 0.9669 - val_loss: 0.1093 - val_accuracy: 0.9635\n",
      "Epoch 4/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0483 - accuracy: 0.9838 - val_loss: 0.1669 - val_accuracy: 0.9502\n",
      "Epoch 5/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0270 - accuracy: 0.9919 - val_loss: 0.1329 - val_accuracy: 0.9551\n",
      "Epoch 6/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0151 - accuracy: 0.9953 - val_loss: 0.1511 - val_accuracy: 0.9565\n",
      "Epoch 7/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0100 - accuracy: 0.9973 - val_loss: 0.1536 - val_accuracy: 0.9579\n",
      "Epoch 8/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1573 - val_accuracy: 0.9572\n",
      "Epoch 9/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 0.1657 - val_accuracy: 0.9579\n",
      "Epoch 10/10\n",
      "179/179 [==============================] - 1s 6ms/step - loss: 0.0056 - accuracy: 0.9986 - val_loss: 0.1563 - val_accuracy: 0.9586\n"
     ]
    }
   ],
   "source": [
    "history = net.fit(train_dataset,\n",
    "                    epochs=10,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data = val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"txt_cnn_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  8344400   \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    multiple                  60100     \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    multiple                  80100     \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    multiple                  100100    \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          multiple                  301       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 8,585,001\n",
      "Trainable params: 8,585,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single gpu usage\n",
    "\n",
    "net = TxtCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "               feature_size, num_filters, kernel_sizes)\n",
    "\n",
    "# Specify the training configuration (optimizer, loss, metrics)\n",
    "\n",
    "net.compile(optimizer=tf.optimizers.Adam(),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11411 samples, validate on 1426 samples\n",
      "11411/11411 [==============================] - 14s 1ms/sample - loss: 0.2334 - accuracy: 0.9102 - val_loss: 0.1672 - val_accuracy: 0.9271\n"
     ]
    }
   ],
   "source": [
    "history = net.fit(train_x, train_y,\n",
    "                    batch_size=32,\n",
    "                    epochs=1,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.backend clear\n",
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
